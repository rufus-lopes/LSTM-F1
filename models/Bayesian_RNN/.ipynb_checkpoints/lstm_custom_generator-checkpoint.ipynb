{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "divided-worcester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices[0])\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "otherwise-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_training_data():\n",
    "\n",
    "    ''' imports training data as a pandas DataFrame '''\n",
    "\n",
    "    dir = '../../SQL_Data/constant_setup'\n",
    "    files = os.listdir(dir)\n",
    "    files = [f for f in files if f.endswith('.sqlite3')]\n",
    "\n",
    "    data = []\n",
    "    for f in files:\n",
    "        path = os.path.join(dir, f)\n",
    "        conn = sqlite3.connect(path)\n",
    "        if os.path.getsize(path) > 10000:\n",
    "            cur = conn.cursor()\n",
    "            cur.execute('SELECT * FROM TrainingData')\n",
    "            df = pd.DataFrame(cur.fetchall())\n",
    "            data.append(df)\n",
    "\n",
    "    names = list(map(lambda x: x[0], cur.description))\n",
    "    df = pd.concat(data)\n",
    "    df.columns = names\n",
    "    df = df.drop(['frameIdentifier','bestLapTime', 'pkt_id', 'packetId', 'SessionTime'], axis=1)\n",
    "    df.set_index('index', inplace=True)\n",
    "\n",
    "    print('Data Imported')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "widespread-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data):\n",
    "    '''\n",
    "\n",
    "    seperates data first by session, then by lap, before padding each array so that\n",
    "    they are all the same length for model input.\n",
    "    Performs test train split also\n",
    "\n",
    "    '''\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    scalers = {}\n",
    "#     sessionUIDs = data.pop('sessionUID')\n",
    "#     lap_number = data.pop('currentLapNum')\n",
    "#     for i in data.columns:\n",
    "#         scaler = MinMaxScaler(feature_range=(0,1))\n",
    "#         s = scaler.fit_transform(data[i].values.reshape(-1,1))\n",
    "#         s = np.reshape(s, len(s))\n",
    "#         scalers['scaler_'+ i ] = scaler\n",
    "#         data[i] = s\n",
    "\n",
    "#     data['sessionUID'] = sessionUIDs\n",
    "#     data['currentLapNum'] = lap_number\n",
    "\n",
    "    session_groups = data.groupby('sessionUID')\n",
    "    training_data = []\n",
    "    target_data = []\n",
    "    total_laps = 0\n",
    "    for s in list(session_groups.groups):\n",
    "        session = session_groups.get_group(s)\n",
    "        lap_groups = session.groupby('currentLapNum')\n",
    "        total_laps += len(lap_groups)\n",
    "        for l in list(lap_groups.groups):\n",
    "            lap = lap_groups.get_group(l)\n",
    "            lap = lap.drop(['sessionUID','currentLapNum'], axis=1)\n",
    "            target_data.append(lap.pop('lap_time_remaining'))\n",
    "            training_data.append(lap)\n",
    "    training = [x.to_numpy() for x in training_data]\n",
    "    target = [y.to_numpy() for y in target_data]\n",
    "    print(f'Total Laps: {total_laps}')\n",
    "\n",
    "    max_timesteps = 10000 # max(training, key=len).shape[0]\n",
    "    num_rows_to_add = [max_timesteps-l.shape[0] for l in training]\n",
    "\n",
    "    training_pad = []\n",
    "    target_pad = []\n",
    "    print(f'max timesteps : {max_timesteps}')\n",
    "\n",
    "    for i in range(len(training)):\n",
    "        rows_to_add = num_rows_to_add[i]\n",
    "\n",
    "        training_arr = training[i]\n",
    "        training_append = np.zeros((rows_to_add, training[0].shape[1]), dtype=float)\n",
    "        training_array = np.vstack((training_arr, training_append))\n",
    "        training_pad.append(training_array)\n",
    "\n",
    "        target_arr = target[i]\n",
    "        target_append = np.zeros((rows_to_add), dtype=float)\n",
    "        target_array = np.concatenate([target_arr, np.zeros(rows_to_add)])\n",
    "        target_pad.append(target_array)\n",
    "\n",
    "    split = int(total_laps*0.9)\n",
    "\n",
    "    X_train = training_pad[:split]\n",
    "    X_test = training_pad[split:]\n",
    "    y_train = target_pad[:split]\n",
    "    y_test = target_pad[split:]\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    print('Data Formatted')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scalers, num_rows_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "protected-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(training, target):\n",
    "    \n",
    "    max_timesteps = 10000 # max(training, key=len).shape[0]\n",
    "    num_rows_to_add = [max_timesteps-l.shape[0] for l in training]\n",
    "    training_pad = []\n",
    "    target_pad = []\n",
    "    print(f'max timesteps : {max_timesteps}')\n",
    "    \n",
    "    for i in range(len(training)):\n",
    "        rows_to_add = num_rows_to_add[i]\n",
    "\n",
    "        training_arr = training[i]\n",
    "        training_append = np.zeros((rows_to_add, training[0].shape[1]), dtype=float)\n",
    "        training_array = np.vstack((training_arr, training_append))\n",
    "        training_pad.append(training_array)\n",
    "\n",
    "        target_arr = target[i].reshape(target[i].shape[0])\n",
    "        target_append = np.zeros((rows_to_add), dtype=float)\n",
    "        target_array = np.concatenate([target_arr, np.zeros(rows_to_add)])\n",
    "        target_pad.append(target_array)\n",
    "    \n",
    "    return training_pad, target_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "    scalers = {}\n",
    "    sessionUIDs = data.pop('sessionUID')\n",
    "    lap_number = data.pop('currentLapNum')\n",
    "    for i in data.columns:\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        s = scaler.fit_transform(data[i].values.reshape(-1,1))\n",
    "        s = np.reshape(s, len(s))\n",
    "        scalers['scaler_'+ i ] = scaler\n",
    "        data[i] = s\n",
    "\n",
    "    data['sessionUID'] = sessionUIDs\n",
    "    data['currentLapNum'] = lap_number\n",
    "    \n",
    "    return data, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pending-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_format(data):\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    \n",
    "    session_groups = data.groupby('sessionUID')\n",
    "    samples = []\n",
    "    targets = []\n",
    "    total_laps = 0\n",
    "    \n",
    "    for s in list(session_groups.groups):\n",
    "        session = session_groups.get_group(s)\n",
    "        lap_groups = session.groupby('currentLapNum')\n",
    "        total_laps += len(lap_groups)\n",
    "        for l in list(lap_groups.groups):\n",
    "            lap = lap_groups.get_group(l)\n",
    "            lap = lap.drop(['sessionUID'], axis=1)\n",
    "            targ = pd.DataFrame(lap.pop('lap_time_remaining'))\n",
    "            targets.append(targ)\n",
    "            samples.append(lap)\n",
    "    \n",
    "    sample_cols = list(samples[0].columns)\n",
    "    target_cols = list(targets[0].columns)\n",
    "\n",
    "    training = [x.to_numpy() for x in samples]\n",
    "    target = [y.to_numpy() for y in targets]\n",
    "    \n",
    "    training, target = pad_data(training, target)\n",
    "    \n",
    "    split = int(total_laps*0.9)\n",
    "\n",
    "    X_train = training[:split]\n",
    "    X_test  = training[split:]\n",
    "    y_train = target[:split]\n",
    "    y_test  = target[split:]\n",
    "    \n",
    "    \n",
    "    Xtrain = np.concatenate(X_train)\n",
    "    Xtest  = np.concatenate(X_test)\n",
    "    Ytrain = np.concatenate(y_train)\n",
    "    Ytest  = np.concatenate(y_test)\n",
    "    \n",
    "    trainX = pd.DataFrame(Xtrain, columns=sample_cols)\n",
    "    testX  = pd.DataFrame(Xtest, columns=sample_cols)\n",
    "    trainY = pd.DataFrame()\n",
    "    testY  = pd.DataFrame()\n",
    "    \n",
    "    trainY['lap_time_remaining'] = Ytrain\n",
    "    testY['lap_time_remaining']  = Ytest\n",
    "    \n",
    "    return trainX, testX, trainY, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rural-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_generator(trainX, testX, trainY, testY):\n",
    "    ''' Creates train and test generators from data for a single lap/sequence'''\n",
    "\n",
    "    look_back = 5\n",
    "    batch_size = 1\n",
    "\n",
    "    train_generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(trainX, trainY, length=look_back, sampling_rate=1, stride=1, batch_size=batch_size)\n",
    "    test_generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(testX, testY, length=look_back, sampling_rate=1, stride=1, batch_size=1)\n",
    "\n",
    "    return train_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inclusive-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_data(trainX, testX, trainY, testY):\n",
    "    trainX, testX, trainY, testY = np.concatenate(trainX), np.concatenate(testX), np.concatenate(trainY), np.concatenate(testY)\n",
    "    return trainX, testX, trainY, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alert-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_generator):\n",
    "    ''' training the model'''\n",
    "    EPOCHS = 1\n",
    "    callback = [EarlyStopping(monitor=\"loss\", min_delta = 0.0001, patience = 10, mode = 'auto', \n",
    "                restore_best_weights=True),]\n",
    "                #ModelCheckpoint('generator_lstm.h5')]\n",
    "    history = model.fit(train_generator, callbacks=callback, shuffle=False, epochs=EPOCHS, batch_size=1)\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "satisfied-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train_generator):\n",
    "\n",
    "    ''' buils model and prints out summary'''\n",
    "    trainX, trainY = train_generator[0]\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    units = 128\n",
    "    epochs = 100\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, LSTM, Dropout, LeakyReLU\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.Masking(mask_value=0., input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "    model.add(LSTM(units, ))\n",
    "    model.add(LeakyReLU(alpha=0.5))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    adam = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='mse', metrics=['mae'])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    print('Model Built')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gentle-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequence_generator(TimeseriesGenerator):\n",
    "\n",
    "    def __init__(self, data, targets, length,\n",
    "                 sampling_rate=1,\n",
    "                 stride=1,\n",
    "                 start_index=0,\n",
    "                 end_index=None,\n",
    "                 shuffle=False,\n",
    "                 reverse=False,\n",
    "                 batch_size=128):\n",
    "\n",
    "        if len(data) != len(targets):\n",
    "            raise ValueError('Data and targets have to be' +\n",
    "                             ' of same length. '\n",
    "                             'Data length is {}'.format(len(data)) +\n",
    "                             ' while target length is {}'.format(len(targets)))\n",
    "\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.length = length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.stride = stride\n",
    "        self.start_index = start_index + length\n",
    "        if end_index is None:\n",
    "            end_index = len(data) - 1\n",
    "        self.end_index = end_index\n",
    "        self.shuffle = shuffle\n",
    "        self.reverse = reverse\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if self.start_index > self.end_index:\n",
    "            raise ValueError('`start_index+length=%i > end_index=%i` '\n",
    "                             'is disallowed, as no part of the sequence '\n",
    "                             'would be left to be used as current step.'\n",
    "                             % (self.start_index, self.end_index))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.shuffle:\n",
    "            rows = np.random.randint(\n",
    "                self.start_index, self.end_index + 1, size=self.batch_size)\n",
    "        else:\n",
    "            i = self.start_index + self.batch_size * self.stride * index\n",
    "            rows = np.arange(i, min(i + self.batch_size *\n",
    "                                    self.stride, self.end_index + 1), self.stride)\n",
    "\n",
    "        samples = np.array([self.data[row - self.length:row:self.sampling_rate]\n",
    "                            for row in rows])\n",
    "        targets = np.array([self.targets[row] for row in rows])\n",
    "\n",
    "        if self.reverse:\n",
    "            return samples[:, ::-1, ...], targets\n",
    "\n",
    "        samples, targets = self.check_overlap(samples, targets)\n",
    "\n",
    "        return samples, targets\n",
    "\n",
    "    def check_overlap(self, samples, targets):\n",
    "        x = samples[0]\n",
    "        lap_col = x[:,-1]\n",
    "        start = lap_col[0]\n",
    "        changes = [not i==start for i in lap_col]\n",
    "        if True in changes:\n",
    "            samples[:,:,:] = 0.\n",
    "        return samples, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "persistent-amendment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max timesteps : 10000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('sample_data.csv')\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = dataframe_format(data)\n",
    "\n",
    "trainX = train_X.to_numpy()\n",
    "testX  = test_X.to_numpy()\n",
    "trainY = train_Y.to_numpy()\n",
    "testY  = test_Y.to_numpy()\n",
    "\n",
    "\n",
    "train_generator, test_generator = single_generator(trainX, testX, trainY, testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "lovely-bennett",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "          1.03774345e+02,  8.24928284e+01,  7.00000000e+00,\n",
      "          0.00000000e+00, -5.12259766e+02, -9.74773407e+00,\n",
      "          3.99145050e+02, -3.69962363e-05,  1.71023747e-03,\n",
      "         -6.66961714e-04,  2.52065134e+00,  1.39009953e-03,\n",
      "          6.95634913e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  3.80200000e+03,  0.00000000e+00,\n",
      "          3.00000000e+01,  3.00000000e+01,  3.00000000e+01,\n",
      "          3.00000000e+01,  8.40000000e+01,  8.40000000e+01,\n",
      "          8.90000000e+01,  8.90000000e+01,  8.50000000e+01,\n",
      "          1.00000000e+00,  6.00000000e+01,  5.47000008e+01,\n",
      "          2.30414772e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  1.80000000e+01,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "          1.03774345e+02,  8.24928284e+01,  7.00000000e+00,\n",
      "          0.00000000e+00, -5.12259766e+02, -9.74773407e+00,\n",
      "          3.99145050e+02, -3.69962363e-05,  1.71023747e-03,\n",
      "         -6.66961714e-04,  2.52065134e+00,  1.39009953e-03,\n",
      "          6.95634913e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  3.80200000e+03,  0.00000000e+00,\n",
      "          3.00000000e+01,  3.00000000e+01,  3.00000000e+01,\n",
      "          3.00000000e+01,  8.40000000e+01,  8.40000000e+01,\n",
      "          8.90000000e+01,  8.90000000e+01,  8.50000000e+01,\n",
      "          1.00000000e+00,  6.00000000e+01,  5.47000008e+01,\n",
      "          2.30414772e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  1.80000000e+01,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "          1.03774345e+02,  8.24928284e+01,  7.00000000e+00,\n",
      "          0.00000000e+00, -5.12259766e+02, -9.74773407e+00,\n",
      "          3.99145050e+02, -3.69962363e-05,  1.71023747e-03,\n",
      "         -6.66961714e-04,  2.52065134e+00,  1.39009953e-03,\n",
      "          6.95634913e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  3.80200000e+03,  0.00000000e+00,\n",
      "          3.00000000e+01,  3.00000000e+01,  3.00000000e+01,\n",
      "          3.00000000e+01,  8.40000000e+01,  8.40000000e+01,\n",
      "          8.90000000e+01,  8.90000000e+01,  8.50000000e+01,\n",
      "          1.00000000e+00,  6.00000000e+01,  5.47000008e+01,\n",
      "          2.30414772e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  1.80000000e+01,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "          1.03774345e+02,  8.24928284e+01,  7.00000000e+00,\n",
      "          0.00000000e+00, -5.12259766e+02, -9.74773407e+00,\n",
      "          3.99145050e+02, -3.69962363e-05,  1.71023747e-03,\n",
      "         -6.66961714e-04,  2.52065134e+00,  1.39009953e-03,\n",
      "          6.95634913e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  3.80200000e+03,  0.00000000e+00,\n",
      "          3.00000000e+01,  3.00000000e+01,  3.00000000e+01,\n",
      "          3.00000000e+01,  8.40000000e+01,  8.40000000e+01,\n",
      "          8.90000000e+01,  8.90000000e+01,  8.50000000e+01,\n",
      "          1.00000000e+00,  6.00000000e+01,  5.47000008e+01,\n",
      "          2.30414772e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  1.80000000e+01,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "          1.03774345e+02,  8.24928284e+01,  7.00000000e+00,\n",
      "          0.00000000e+00, -5.12259766e+02, -9.74773407e+00,\n",
      "          3.99145050e+02, -3.69962363e-05,  1.71023747e-03,\n",
      "         -6.66961714e-04,  2.52065134e+00,  1.39009953e-03,\n",
      "          6.95634913e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  3.80200000e+03,  0.00000000e+00,\n",
      "          3.00000000e+01,  3.00000000e+01,  3.00000000e+01,\n",
      "          3.00000000e+01,  8.40000000e+01,  8.40000000e+01,\n",
      "          8.90000000e+01,  8.90000000e+01,  8.50000000e+01,\n",
      "          1.00000000e+00,  6.00000000e+01,  5.47000008e+01,\n",
      "          2.30414772e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  1.80000000e+01,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]]), array([[103.7743454]]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# data = import_training_data()\n",
    "\n",
    "\n",
    "# trainX, testX, trainY, testY, scalers, num_rows_to_add = format_data(data)\n",
    "\n",
    "# train_X, test_X, train_Y, test_Y = concat_data(trainX, testX, trainY, testY)\n",
    "\n",
    "# look_back = 5\n",
    "# batch_size=1\n",
    "\n",
    "# train_generator = sequence_generator(train_X, train_Y, length=look_back, sampling_rate=1, stride=1, batch_size=batch_size)\n",
    "\n",
    "# test_generator = sequence_generator(test_X, test_Y, length=look_back, sampling_rate=1, stride=1, batch_size=batch_size)\n",
    "\n",
    "# model = build_model(train_generator)\n",
    "\n",
    "\n",
    "# history, model = train_model(model, train_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, test_generator, scalers):\n",
    "    preds = model.predict(test_generator)\n",
    "    return preds\n",
    " \n",
    "pred = make_predictions(model, test_generator, scalers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.ravel()\n",
    "testY = testY.ravel()\n",
    "print(f'predictor shape {pred.shape}')\n",
    "print(f'test shape {testY.shape}')\n",
    "test_y = testY[:-5]\n",
    "print(f'test shape new {test_y.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = test_y # scalers['scaler_lap_time_remaining'].inverse_transform(test_y.reshape(-1,1)).ravel()\n",
    "print(truth.shape)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_prev = 0\n",
    "err = pd.Series(truth-pred)\n",
    "test_df = pd.DataFrame()\n",
    "test_df['truth'] = truth\n",
    "test_df['error'] = err\n",
    "test_df['prediction'] = preds\n",
    "print(test_df.info())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_prev = 0\n",
    "err = truth-pred\n",
    "for j in range(trainX.shape[1], len(pred), trainX.shape[1]):\n",
    "    plt.plot(err[j_prev:j])\n",
    "    j_prev = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = sequence_generator(train_X, train_Y, length=look_back, sampling_rate=1, stride=1, batch_size=batch_size)\n",
    "count = 0\n",
    "for i in range(len(gen)):\n",
    "    x,y = gen[i]\n",
    "    if np.all(x == 10.):\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-scanner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
